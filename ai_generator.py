"""Generate viral content using OpenRouter with batching support."""

from __future__ import annotations

import json
import os
import time
from typing import Dict, List, Optional

import config
from openrouter_client import OpenRouterClient, AllKeysExhaustedError

logger = config.get_logger("ai_generator")


# Batch prompt template - generates content for multiple articles in one call
# Based on: "Ã‰crire des posts courts et engageants sur la tech et le gaming"
_DEFAULT_BATCH_PROMPT = """Ø£Ù†Øª Ù…Ù†Ø´Ø¦ Ù…Ø­ØªÙˆÙ‰ ÙÙŠØ±ÙˆØ³ÙŠ Ù…Ø­ØªØ±Ù Ù„ÙÙŠØ³Ø¨ÙˆÙƒØŒ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ§Ù„Ø£Ù„Ø¹Ø§Ø¨ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠ.

ðŸŽ¯ Ø§Ù„Ù…Ø¨Ø¯Ø£ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: "Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ = ÙƒÙ„ Ø´ÙŠØ¡" - ÙŠØ¬Ø¨ Ø£Ù† ØªÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ø±Ø¦ Ø¹Ù† Ø§Ù„ØªÙ…Ø±ÙŠØ± ÙÙˆØ±Ø§Ù‹!

ðŸ“° Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ({count} Ù…Ù‚Ø§Ù„):
{articles_json}

Ù„ÙƒÙ„ Ù…Ù‚Ø§Ù„ØŒ Ø£Ù†Ø´Ø¦ Ù…Ø­ØªÙˆÙ‰ ÙŠØªØ¨Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©:

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ”¥ 1. Ø§Ù„Ø§ÙØªØªØ§Ø­ÙŠØ© (HOOK) - Ø£Ù‡Ù… Ø¬Ø²Ø¡!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ø®ØªØ± Ù†ÙˆØ¹Ø§Ù‹ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª:
â€¢ Ø³Ø¤Ø§Ù„ ØµØ§Ø¯Ù…: "Ù‡Ù„ ØªØµØ¯Ù‚ Ø£Ù†...ØŸ"
â€¢ Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ù…ÙØ§Ø¬Ø¦Ø©: "90% Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù„Ø§ ÙŠØ¹Ø±ÙÙˆÙ† Ù‡Ø°Ø§!"
â€¢ ØªØµØ±ÙŠØ­ Ø¬Ø±ÙŠØ¡: "Ù†Ø³ÙŠØª ÙƒÙ„ Ù…Ø§ ØªØ¹Ø±ÙÙ‡ Ø¹Ù†..."
â€¢ Ø¥ÙŠÙ…ÙˆØ¬ÙŠ + ØªØ­Ø°ÙŠØ±: "ðŸš¨ Ø®Ø¨Ø± Ø¹Ø§Ø¬Ù„!"
â€¢ Ù‚ØµØ© Ø´Ø®ØµÙŠØ©: "Ø§ÙƒØªØ´ÙØª Ù„Ù„ØªÙˆ Ø´ÙŠØ¦Ø§Ù‹ ØºÙŠÙ‘Ø± ÙƒÙ„ Ø´ÙŠØ¡..."

âš ï¸ Ù…Ù‡Ù…: Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø§Ø±ÙƒØ§Øª ØªØ¨Ù‚Ù‰ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© (ChatGPT, OpenAI, Tesla, ASUS, Apple...)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ’Ž 2. Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (BODY) - Ù‚ÙŠÙ…Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Ù‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ©
â€¢ "Ù‡Ù„ ØªØ¹Ù„Ù… Ø£Ù†..." Ø£Ùˆ "Ù†ØµÙŠØ­Ø© Ø³Ø±ÙŠØ¹Ø©:..."
â€¢ Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (ÙƒØ£Ù†Ùƒ ØªØªØ­Ø¯Ø« Ù…Ø¹ ØµØ¯ÙŠÙ‚)
â€¢ Ø¬Ù…Ù„ Ù‚ØµÙŠØ±Ø© ÙˆÙ…Ø¤Ø«Ø±Ø©
â€¢ ÙÙ‚Ø±Ø§Øª Ù‚ØµÙŠØ±Ø© (2-3 Ø£Ø³Ø·Ø±)
â€¢ 2-3 Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ù…Ù†Ø§Ø³Ø¨Ø© ðŸŽ®ðŸ¤–ðŸ’¡

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“£ 3. Ø¯Ø¹ÙˆØ© Ù„Ù„ØªÙØ§Ø¹Ù„ (CTA) - Ø¶Ø±ÙˆØ±ÙŠØ©!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ø®ØªØ± Ø³Ø¤Ø§Ù„Ø§Ù‹ ÙŠØ¯Ø¹Ùˆ Ù„Ù„ØªØ¹Ù„ÙŠÙ‚:
â€¢ "Ù…Ø§ Ø±Ø£ÙŠÙƒÙ…ØŸ Ø´Ø§Ø±ÙƒÙˆÙ†Ø§ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª! ðŸ’¬"
â€¢ "Ù‡Ù„ Ø¬Ø±Ø¨ØªÙ… Ù‡Ø°Ø§ Ù…Ù† Ù‚Ø¨Ù„ØŸ"
â€¢ "Ù…Ù† Ù…Ø¹ÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø±Ø£ÙŠØŸ ðŸ™‹â€â™‚ï¸"
â€¢ "ØªØ§Øº Ø´Ø®Øµ ÙŠØ­ØªØ§Ø¬ ÙŠØ¹Ø±Ù Ù‡Ø°Ø§!"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#ï¸âƒ£ 4. Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5-7 Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª Ù…Ø²ÙŠØ¬ Ù…Ù†:
- Ø¹Ø±Ø¨ÙŠØ©: #ØªÙ‚Ù†ÙŠØ© #Ø§Ù„Ø°ÙƒØ§Ø¡_Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ #Ø£Ù„Ø¹Ø§Ø¨
- Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù„Ù„Ù…Ø§Ø±ÙƒØ§Øª: #ChatGPT #OpenAI #Tesla

Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ):
[
  {{
    "article_index": 0,
    "text_post": {{
      "hook": "ðŸš¨ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø°ÙŠ Ø³ÙŠØºÙŠØ± ÙƒÙ„ Ø´ÙŠØ¡!...",
      "body": "Ù‡Ù„ ØªØ¹Ù„Ù… Ø£Ù†... [Ù…Ø­ØªÙˆÙ‰ Ù‚ÙŠÙ… ÙˆÙ…ÙÙŠØ¯]...",
      "cta": "Ù…Ø§ Ø±Ø£ÙŠÙƒÙ…ØŸ Ø´Ø§Ø±ÙƒÙˆÙ†Ø§ ØªØ¬Ø±Ø¨ØªÙƒÙ…! ðŸ’¬",
      "hashtags": ["#Ø§Ù„Ø°ÙƒØ§Ø¡_Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "#ØªÙ‚Ù†ÙŠØ©", "#ChatGPT"]
    }}
  }}
]
"""


# Single article prompt (fallback) - v2.0 simplified, no Reels
# Supports style parameter: emotional, news, casual, motivation
_DEFAULT_SINGLE_PROMPT = """Ø£Ù†Øª Ù…Ù†Ø´Ø¦ Ù…Ø­ØªÙˆÙ‰ ÙÙŠØ±ÙˆØ³ÙŠ Ù…Ø­ØªØ±Ù Ù„ÙÙŠØ³Ø¨ÙˆÙƒ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ§Ù„Ø£Ù„Ø¹Ø§Ø¨.

ðŸŽ¯ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©: "Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ ÙŠØ­Ø¯Ø¯ ÙƒÙ„ Ø´ÙŠØ¡!" - Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù‚Ø§Ø±Ø¦ ÙŠØªÙˆÙ‚Ù ÙÙˆØ±Ø§Ù‹!

ðŸ“° Ø§Ù„Ù…Ù‚Ø§Ù„:
Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {title}
Ø§Ù„Ù…Ù„Ø®Øµ: {summary}

ðŸ“ Ø§ÙƒØªØ¨ Ù…Ù†Ø´ÙˆØ±Ø§Ù‹ ÙŠØªØ¨Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ø¨Ù†ÙŠØ©:

1. ðŸ”¥ HOOK (Ø§ÙØªØªØ§Ø­ÙŠØ© ØµØ§Ø¯Ù…Ø© - 10 ÙƒÙ„Ù…Ø§Øª ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)
2. ðŸ’Ž BODY (Ù…Ø­ØªÙˆÙ‰ Ù‚ÙŠÙ… - 50-80 ÙƒÙ„Ù…Ø©)
3. ðŸ“£ CTA (Ø¯Ø¹ÙˆØ© Ù„Ù„ØªÙØ§Ø¹Ù„)
4. #ï¸âƒ£ HASHTAGS (5-7 Ù…Ø²ÙŠØ¬ Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)

âš ï¸ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø§Ø±ÙƒØ§Øª Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: ChatGPT, Tesla, Apple, OpenAI

Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø·:
{{
  "hook": "ðŸš¨ [Ø§ÙØªØªØ§Ø­ÙŠØ© ØµØ§Ø¯Ù…Ø©]...",
  "body": "[Ù…Ø­ØªÙˆÙ‰ Ù‚ÙŠÙ…]...",
  "cta": "Ù…Ø§ Ø±Ø£ÙŠÙƒÙ…ØŸ ðŸ’¬",
  "hashtags": ["#ØªÙ‚Ù†ÙŠØ©", "#ChatGPT"]
}}
"""


def get_prompts() -> Dict[str, str]:
    """Return current prompt templates (custom env override or defaults)."""
    return {
        "batch": os.environ.get("CUSTOM_BATCH_PROMPT", "").strip() or _DEFAULT_BATCH_PROMPT,
        "single": os.environ.get("CUSTOM_SINGLE_PROMPT", "").strip() or _DEFAULT_SINGLE_PROMPT,
    }


def set_prompts(batch: Optional[str] = None, single: Optional[str] = None) -> None:
    """Update prompt env vars at runtime."""
    if batch is not None:
        os.environ["CUSTOM_BATCH_PROMPT"] = batch
    if single is not None:
        os.environ["CUSTOM_SINGLE_PROMPT"] = single


# Module-level aliases (for backward compat + runtime override)
BATCH_PROMPT_TEMPLATE = get_prompts()["batch"]
SINGLE_PROMPT_TEMPLATE = get_prompts()["single"]


def get_ai_client_instance() -> GeminiClient:
    """Get configured AI client (Gemini default, OpenRouter fallback)."""
    return get_ai_client()


def fix_json_string(text: str) -> str:
    """Attempt to fix common JSON issues from LLM output."""
    import re

    # Remove trailing commas before } or ]
    text = re.sub(r",\s*([}\]])", r"\1", text)

    # Fix unescaped newlines in strings
    # This is a simplified fix - handles common cases
    lines = text.split("\n")
    fixed_lines = []
    in_string = False
    for line in lines:
        # Count unescaped quotes to track if we're in a string
        quote_count = len(re.findall(r'(?<!\\)"', line))
        if in_string:
            # If we're continuing a string from previous line, escape this line
            fixed_lines.append(line.replace("\n", "\\n"))
        else:
            fixed_lines.append(line)
        # Toggle in_string if odd number of quotes
        if quote_count % 2 == 1:
            in_string = not in_string

    return "\n".join(fixed_lines)


def parse_json_response(text: str) -> any:
    """Extract and parse JSON from model response with error recovery."""
    stripped = text.strip()

    # Remove markdown code blocks if present
    if stripped.startswith("```"):
        lines = stripped.split("\n")
        # Find start and end of code block
        start_idx = 1 if lines[0].startswith("```") else 0
        end_idx = len(lines)
        for i in range(len(lines) - 1, -1, -1):
            if lines[i].strip() == "```":
                end_idx = i
                break
        stripped = "\n".join(lines[start_idx:end_idx])

    # Find JSON array or object
    start_array = stripped.find("[")
    start_obj = stripped.find("{")

    json_text = None

    if start_array != -1 and (start_obj == -1 or start_array < start_obj):
        # It's an array
        end = stripped.rfind("]")
        if end != -1:
            json_text = stripped[start_array : end + 1]
    elif start_obj != -1:
        # It's an object
        end = stripped.rfind("}")
        if end != -1:
            json_text = stripped[start_obj : end + 1]

    if json_text is None:
        raise ValueError(f"No valid JSON found in response: {stripped[:200]}...")

    # Try parsing directly first
    try:
        return json.loads(json_text)
    except json.JSONDecodeError:
        pass

    # Try fixing common issues
    try:
        fixed = fix_json_string(json_text)
        return json.loads(fixed)
    except json.JSONDecodeError:
        pass

    # Last resort: try to extract individual objects if array fails
    if json_text.startswith("["):
        try:
            # Try to find complete objects
            import re

            objects = re.findall(r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}", json_text)
            if objects:
                results = []
                for obj in objects:
                    try:
                        results.append(json.loads(obj))
                    except json.JSONDecodeError:
                        continue
                if results:
                    logger.warning("Recovered %d objects from malformed JSON array", len(results))
                    return results
        except Exception:
            pass

    raise ValueError(f"Failed to parse JSON after recovery attempts: {json_text[:300]}...")


def generate_batch(articles: List[dict], client: Optional[GeminiClient] = None) -> List[Dict]:
    """
    Generate content for multiple articles in a single API call.

    Args:
        articles: List of article dicts with 'title' and 'content' keys
        client: Optional AI client (creates new one if not provided)

    Returns:
        List of generated content dicts
    """
    if not articles:
        return []

    if client is None:
        client = get_ai_client_instance()

    # Format articles for prompt
    articles_for_prompt = []
    for i, article in enumerate(articles):
        articles_for_prompt.append(
            {
                "index": i,
                "title": article.get("title", ""),
                "summary": (article.get("content") or "")[:500],  # Limit summary length
            }
        )

    # A/B testing: select variant if enabled
    variant_id = None
    active_template = BATCH_PROMPT_TEMPLATE
    try:
        from ab_testing import pick_variant
        variant = pick_variant()
        if variant:
            active_template = variant["template"]
            variant_id = variant["id"]
            logger.info("A/B: using variant '%s'", variant["name"])
    except ImportError:
        pass

    prompt = active_template.format(
        count=len(articles),
        articles_json=json.dumps(articles_for_prompt, ensure_ascii=False, indent=2),
    )

    try:
        response = client.generate(prompt, max_tokens=2000, temperature=0.7)
        results = parse_json_response(response)

        if not isinstance(results, list):
            results = [results]

        # Tag results with variant_id for A/B tracking
        if variant_id:
            for r in results:
                if isinstance(r, dict):
                    r["variant_id"] = variant_id

        logger.info("Batch generation successful: %d articles processed", len(results))
        return results

    except AllKeysExhaustedError:
        logger.error("All API keys exhausted - wait for rate limit reset")
        raise
    except json.JSONDecodeError as exc:
        logger.error("Failed to parse JSON response: %s", exc)
        raise
    except Exception as exc:
        logger.error("Batch generation failed: %s", exc)
        raise


def generate_single(
    article: dict, client: Optional[GeminiClient] = None
) -> Dict:
    """
    Generate content for a single article (fallback method).

    Args:
        article: Article dict with 'title' and 'content'
        client: Optional AI client

    Returns:
        Generated content dict
    """
    if client is None:
        client = get_ai_client_instance()

    prompt = SINGLE_PROMPT_TEMPLATE.format(
        title=article.get("title", ""),
        summary=(article.get("content") or "")[:500],
    )

    response = client.generate(prompt, max_tokens=1024, temperature=0.7)
    return parse_json_response(response)


def save_processed_content(
    article_id: str, post_type: str, payload: Dict, article_title: str = ""
) -> None:
    """Save generated content to Supabase and generate social image."""
    db_client = config.get_supabase_client()

    # Generate social media image
    image_path = ""
    arabic_text = ""
    try:
        from image_pipeline import generate_social_post

        ai_client = get_ai_client_instance()
        image_path, arabic_text = generate_social_post(
            article_id=article_id,
            title=article_title or payload.get("hook", ""),
            client=ai_client,
        )
        logger.info("Generated social image: %s", image_path)
    except Exception as e:
        logger.warning("Image generation failed (continuing): %s", e)

    record = {
        "article_id": article_id,
        "post_type": post_type,
        "generated_text": payload.get("body", ""),
        "hashtags": payload.get("hashtags", []),
        "hook": payload.get("hook"),
        "call_to_action": payload.get("cta"),
        "target_audience": "AR",  # v2.0: Target Arabic audience
        "image_path": image_path,
        "arabic_text": arabic_text,
    }

    db_client.table("processed_content").insert(record).execute()
    logger.debug("Saved processed content for article %s", article_id)


def mark_article_processed(article_id: str) -> None:
    """Mark article as processed in database."""
    client = config.get_supabase_client()
    client.table("raw_articles").update({"status": "processed"}).eq("id", article_id).execute()


def process_pending_articles(limit: int = 10, batch_size: int = 5) -> int:
    """
    Process pending articles with batching for efficiency.

    Args:
        limit: Maximum number of articles to process
        batch_size: Number of articles per batch (default 5)

    Returns:
        Number of articles processed
    """
    db_client = config.get_supabase_client()

    # Fetch pending articles
    response = (
        db_client.table("raw_articles")
        .select("id,title,content")
        .eq("status", "pending")
        .limit(limit)
        .execute()
    )

    rows = response.data or []
    if not rows:
        logger.info("No pending articles to process")
        return 0

    logger.info("Processing %d pending articles in batches of %d", len(rows), batch_size)

    ai_client = get_ai_client_instance()
    processed = 0

    # Process in batches
    for i in range(0, len(rows), batch_size):
        batch = rows[i : i + batch_size]

        try:
            # Generate content for batch
            results = generate_batch(batch, client=ai_client)

            # Ensure results is a list
            if not isinstance(results, list):
                results = [results]

            # Save results
            for j, result in enumerate(results):
                if j >= len(batch):
                    break

                # Handle case where result might be a string or unexpected type
                if not isinstance(result, dict):
                    logger.warning("Skipping non-dict result: %s", type(result))
                    continue

                article = batch[j]
                article_id = article["id"]

                try:
                    # Save text/photo post - v2.0 simplified (no Reels)
                    text_data = result.get("text_post") or result
                    if isinstance(text_data, dict) and (
                        text_data.get("hook") or text_data.get("body")
                    ):
                        save_processed_content(
                            article_id, "text", text_data, article_title=article.get("title", "")
                        )

                    mark_article_processed(article_id)
                    processed += 1
                except Exception as item_exc:
                    logger.error("Failed to save content for article %s: %s", article_id, item_exc)
                    continue

            # Rate limiting pause between batches
            if i + batch_size < len(rows):
                time.sleep(config.REQUEST_SLEEP_SECONDS)

        except AllKeysExhaustedError:
            logger.error("All API keys exhausted, stopping batch processing")
            break
        except Exception as exc:
            logger.error("Batch processing failed: %s", exc)
            # Continue with next batch
            continue

    logger.info("Processed %d articles total", processed)
    return processed


if __name__ == "__main__":
    process_pending_articles()
