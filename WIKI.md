# üìñ Content Factory - The Deep Dive Documentation (v2.0)

> **Documentation Technique & Op√©rationnelle Compl√®te**
> Ce document est la "Bible" du projet. Il remplace toute documentation pr√©c√©dente.

---

## üìë Table des Mati√®res

1.  [Architecture Syst√®me](#1-architecture-syst√®me)
2.  [Configuration Bible (.env)](#2-configuration-bible-env)
3.  [R√©f√©rence Base de Donn√©es](#3-r√©f√©rence-base-de-donn√©es)
4.  [Documentation des Modules (API Internals)](#4-documentation-des-modules-api-internals)
5.  [Flux Op√©rationnels (Workflows)](#5-flux-op√©rationnels-workflows)
6.  [D√©pannage & FAQ](#6-d√©pannage--faq)

---

## 1. Architecture Syst√®me

Le projet est con√ßu comme un pipeline lin√©aire autonome. Il s'ex√©cute s√©quentiellement pour garantir la qualit√© de chaque √©tape avant de passer √† la suivante.

### Diagramme de Flux (D√©taill√©)

```mermaid
graph TD
    subgraph INPUT
    A[Cron Job / CLI] -->|Trigger| B(Main Orchestrator)
    end

    subgraph PHASE 1: ACQUISITION
    B -->|Calls| Scraper[scraper.py]
    Scraper -->|Fetch| RSS[RSS Feeds]
    Scraper -->|Fetch| NewsData[NewsData IO]
    Scraper -->|Fetch| HN[HackerNews]
    Scraper -->|Store| DB[(Database: raw_articles)]
    end

    subgraph PHASE 2: G√âN√âRATION (Unified Worker)
    DB -->|Read Pending| Creator[unified_content_creator.py]
    Creator -->|1. Pick Trending| TrendAlgo{Viral Score}
    Creator -->|2. Generate Text| AI[Gemini / OpenRouter]
    AI -->|JSON| ContentStruct
    Creator -->|3. Get Image| ImgPipe[image_pipeline.py]
    ImgPipe -->|Source| Pexels/Unsplash
    ImgPipe -->|Overlay| Canvas[Pillow Generator]
    ContentStruct -->|Combine| Processor
    Canvas -->|Combine| Processor
    Processor -->|Store| DB2[(Database: processed_content)]
    end

    subgraph PHASE 3: PLANIFICATION & PUBLICATION
    DB2 -->|Read Pool| Sched[scheduler.py]
    Sched -->|Human Logic| Slots[Time Slots Generator]
    Slots -->|Insert| DB3[(Database: scheduled_posts)]
    
    Clock -->|Check Due| Pub[publisher.py]
    DB3 -->|Read Due| Pub
    Pub -->|POST| FB[Facebook Graph API]
    Pub -->|Update Status| DB4[(Database: published_posts)]
    end
```

---

## 2. Configuration Bible (`.env`)

Toutes les variables d'environnement support√©es par `config.py`.

| Variable | Requis | Valeur par d√©faut | Description |
| :--- | :---: | :--- | :--- |
| **DATABASE** |
| `DB_MODE` | Non | `sqlite` | `sqlite` (local) ou `supabase` (cloud). |
| `SUPABASE_URL` | Si cloud | - | URL du projet Supabase. |
| `SUPABASE_KEY` | Si cloud | - | Cl√© ANON (publique) Supabase. |
| **AI GENERATION** |
| `GEMINI_API_KEY` | **OUI** | - | Cl√© API Google Gemini (Primaire). |
| `OPENROUTER_API_KEY_1` | Non | - | Cl√© de secours pour OpenRouter. |
| `OPENROUTER_API_KEY_2` | Non | - | Cl√© de secours #2. |
| **SOCIAL MEDIA** |
| `FACEBOOK_ACCESS_TOKEN` | **OUI** | - | Token "Page Access Token" (Long-lived). |
| `FACEBOOK_PAGE_ID` | **OUI** | - | ID num√©rique de la page Facebook. |
| **IMAGES** |
| `PEXELS_API_KEY` | Non | - | Cl√© Pexels. Si vide, fallback sur Unsplash/LoremPicsum. |
| `PIXABAY_API_KEY` | Non | - | Cl√© Pixabay (Backup). |
| **SYSTEM** |
| `HTTP_TIMEOUT_SECONDS` | Non | `20` | Timeout pour les requ√™tes HTTP. |
| `REQUEST_SLEEP_SECONDS` | Non | `2` | Pause entre les requ√™tes (Rate Limit). |
| `DASHBOARD_API_KEY` | Non | - | S√©curisation de l'API Flask. |

---

## 3. R√©f√©rence Base de Donn√©es

Le syst√®me supporte un sch√©ma hybride. Voici la sp√©cification exacte des tables critiques.

### `raw_articles`
Stocke les donn√©es brutes scrap√©es.
- `id` (UUID/TEXT): Cl√© primaire.
- `url` (TEXT UNIQUE): URL de l'article pour d√©duplication.
- `title` (TEXT): Titre original.
- `source_name` (TEXT): `techcrunch`, `verge`, etc.
- `status` (TEXT): `pending` (√† traiter), `processed` (termin√©), `rejected` (ignor√©).
- `virality_score` (INT): Score calcul√© (0-10) bas√© sur la fra√Æcheur et la source.

### `processed_content`
Le contenu pr√™t √† l'emploi.
- `post_type` (TEXT): `text` (v2.0) ou `reel` (legacy).
- `generated_text` (TEXT): Le corps du post Facebook.
- `hook` (TEXT): La premi√®re ligne ("accroche") du post.
- `hashtags` (JSON/TEXT): Tableau de tags.
- `image_path` (TEXT): Chemin local ou URL de l'image finale g√©n√©r√©e.
- `arabic_text` (TEXT): Le texte court incrust√© sur l'image.

### `scheduled_posts`
La file d'attente de publication.
- `scheduled_time` (TIMESTAMP): Date/Heure exacte de publication (UTC).
- `status` (TEXT): `scheduled`, `published`, `failed`.
- `priority` (INT): 1-10.
- `timezone` (TEXT): Zone cible (ex: `America/New_York` pour calculer les heures de pointe).

---

## 4. Documentation des Modules (API Internals)

### üß† `ai_generator.py` (Le Cerveau)
Ce module g√®re toute la logique de g√©n√©ration de texte.
*   **`generate_batch(articles, client)`**:
    *   Prend une liste d'articles.
    *   Utilise `BATCH_PROMPT_TEMPLATE` pour demander √† l'IA de traiter N articles en un seul appel (√©conomie de tokens).
    *   G√®re le parsing robuste du JSON de r√©ponse avec `fix_json_string` (r√©pare les erreurs courantes des LLM comme les virgules en trop).
*   **`parse_json_response(text)`**:
    *   Fonction critique qui nettoie la r√©ponse de l'IA (supprime les blocs markdown ` ```json `).
    *   Tente plusieurs strat√©gies de r√©cup√©ration si le JSON est malform√©.

### üè≠ `unified_content_creator.py` (L'Orchestrateur)
Coordonne le pipeline pour un contenu unique.
*   **`create_and_publish(...)`**:
    1.  Trouve un sujet tendance (`find_trending_topic`).
    2.  V√©rifie les doublons avec `check_duplicate` (Logique floue de similarit√© de texte).
    3.  Appelle `generate_complete_content`.
    4.  Lance le pipeline image (`find_matching_image` -> `compose_canvas`).
    5.  Sauvegarde en DB.
    6.  Publie si le flag `publish=True`.

### üóÑÔ∏è `database.py` (L'Abstraction)
Ce fichier est une prouesse de compatibilit√©.
*   **`get_db()`**: Factory qui retourne soit `SQLiteDB` soit `SupabaseWrapper`.
*   **`SQLiteTable`**: Une classe qui impl√©mente *exactement* la m√™me interface que le client Python Supabase (`select`, `eq`, `insert`, `execute`).
    *   *Pourquoi ?* Cela permet de changer de backend DB en changeant une seule ligne dans `.env` sans toucher au reste du code.

### üìÜ `scheduler.py` (Le Planificateur)
Logique humaine pour √©viter les bans.
*   **`build_slots_for_day`**: G√©n√®re des cr√©neaux bas√©s sur `PEAK_HOURS`.
*   **`enforce_min_gap_random`**:
    *   Assure qu'il n'y a pas deux posts trop rapproch√©s.
    *   Utilise un intervalle al√©atoire (ex: entre 2h et 4h) pour simuler un comportement humain naturel et non robotique.

---

## 5. Flux Op√©rationnels (Workflows)

### Workflow A: Ajout d'une nouvelle source de News
Pour ajouter un nouveau flux RSS :
1.  Ouvrir `scraper.py`.
2.  Localiser la liste `RSS_FEEDS`.
3.  Ajouter l'URL : `RSS_FEEDS.append("https://nouvelle-source.com/rss")`.
4.  Relancer `python main.py scrape`.

### Workflow B: Debugging d'une g√©n√©ration √©chou√©e
Si un post ne se g√©n√®re pas :
1.  V√©rifier `logs/pipeline.log`.
2.  Chercher "JSON parse error" ou "API key exhausted".
3.  Si JSON error : V√©rifier le prompt dans `ai_generator.py` (peut-√™tre trop complexe).
4.  Si API error : V√©rifier le quota Gemini ou ajouter une cl√© OpenRouter.

### Workflow C: Reset complet de la base de donn√©es (Local)
En cas de corruption ou pour repartir √† z√©ro :
1.  Arr√™ter le script.
2.  Supprimer le fichier `content_factory.db`.
3.  Relancer n'importe quel script (`main.py` ou autre).
4.  `database.py` recr√©era automatiquement toutes les tables vides au d√©marrage.

---

## 6. D√©pannage & FAQ

**Q: Pourquoi les images sont-elles g√©n√©riques ?**
R: Si Pexels/Pixabay √©chouent (ou pas de cl√©), le syst√®me utilise Unsplash (mot-cl√©) ou LoremPicsum comme fallback ultime. Ajoutez une cl√© `PEXELS_API_KEY` pour de meilleurs r√©sultats.

**Q: Facebook bloque mes posts (Rate Limit).**
R: Le syst√®me a par d√©faut une pause de 2 secondes (`REQUEST_SLEEP_SECONDS`) et un "jitter" dans le scheduler. Si le blocage persiste, augmentez `REQUEST_SLEEP_SECONDS` √† 10 ou 30 dans `.env`.

**Q: Comment voir les posts programm√©s ?**
R: Lancez le dashboard (`python dashboard_app.py`) et allez sur `localhost:5000`, ou inspectez la table `scheduled_posts` via un outil SQLite.

**Q: Puis-je utiliser MySQL ou PostgreSQL directement ?**
R: Le code est optimis√© pour SQLite ou Supabase (Postgres HTTP API). Pour un Postgres local standard, il faudrait adapter `database.py` pour utiliser `psycopg2` ou `sqlalchemy`.

---
*Documentation g√©n√©r√©e automatiquement par Deep Dive Analyst - Content Factory Team*
